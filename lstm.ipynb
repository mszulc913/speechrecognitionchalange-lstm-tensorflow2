{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import codecs\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import IPython.display as ipd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "DATA_PATH = Path('data')\n",
    "\n",
    "ALLOWED_LABELS = set([\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \n",
    "                      \"go\", \"zero\", \"one\", \"two\", \"three\", \"four\",\n",
    "                      \"five\", \"six\", \"seven\", \"eight\", \"nine\"])\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "\n",
    "def get_info_row(x):\n",
    "    \"\"\"Splits one row int path, label and user_id\"\"\"\n",
    "    path = x.split('\\n')[0]\n",
    "    label = transform_label(path.split('/')[0])\n",
    "    user_id = path.split('/')[1].split('_')[0]\n",
    "    return path, label, user_id\n",
    "\n",
    "def transform_label(label):\n",
    "    if label not in ALLOWED_LABELS:\n",
    "        label = 'unknown'\n",
    "    return label\n",
    "\n",
    "def load_test_data_info(data_path):\n",
    "    \"\"\"Returns list of tuples in format (path, label, user_id)\"\"\"\n",
    "    with open(data_path / \"validation_list.txt\", \"r\") as f:\n",
    "        validation_list = [x for x in f.readlines()]\n",
    "    with open(data_path / \"testing_list.txt\", \"r\") as f:\n",
    "        testing_list = [x for x in f.readlines()]\n",
    "\n",
    "    test_info = validation_list + testing_list\n",
    "    test_info = map(lambda x: get_info_row(x), test_info)\n",
    "    return list(test_info)\n",
    "\n",
    "def get_train_test_data_info(data_path):\n",
    "    \"\"\"Returns list of tuples in format (path, label, user_id)\"\"\"\n",
    "    test_info = load_test_data_info(data_path)\n",
    "    \n",
    "    train_info = []\n",
    "    paths = set([x[0] for x in test_info])\n",
    "    for file_path in data_path.glob('*/*.wav'):\n",
    "        path = str(file_path).split(str(data_path) + '/')[1]\n",
    "        if path not in paths:\n",
    "            train_info.append((path,\n",
    "                               transform_label(path.split('/')[0]),\n",
    "                               path.split('/')[1].split('_')[0]))\n",
    "\n",
    "    return train_info, test_info\n",
    "\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    \"\"\"Computes log spectogram of a wav sample\"\"\"\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.astype(np.float32) + eps)\n",
    "    \n",
    "def pad_audio(samples, length=16000):\n",
    "    \"\"\"Pads samples that have less than 'length' samples with 0\"\"\"\n",
    "    if len(samples) >= length:\n",
    "        return samples\n",
    "    else:\n",
    "        return np.pad(samples, pad_width=(length - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, length=16000):\n",
    "    \"\"\"Chops audio to have 'length' samples\"\"\"\n",
    "    return samples[:length]\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    \"\"\"Applies transformations to the file (like computing spectrogram)\"\"\"\n",
    "    sample_rate, samples = wavfile.read(file_path)\n",
    "    samples = pad_audio(samples)\n",
    "    samples = chop_audio(samples)\n",
    "    \n",
    "    _, _, spectrogram = log_specgram(samples, sample_rate)\n",
    "    return spectrogram.T\n",
    "\n",
    "def get_plot_confusion_matrix(cmatrix):\n",
    "    \"\"\"Plots confusion matrix heatmap\"\"\"\n",
    "    df_cm = pd.DataFrame(cmatrix, index = list(label_to_index.keys()),\n",
    "                  columns = list(label_to_index.keys()))\n",
    "    fig = plt.figure(figsize = (16, 9))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\n",
    "    return fig\n",
    "\n",
    "def get_summary(probs, true_labels):\n",
    "    \"\"\"Returns dict with accuracy, confusion matrix plot, binary probabilities per class and AUC per class\"\"\"\n",
    "    pred_labels = np.argmax(probs, axis=1)\n",
    "    accuracy = sum(pred_labels == true_labels) / len(test_labels)\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    cmatrix = confusion_matrix(true_labels, pred_labels)\n",
    "    plot = get_plot_confusion_matrix(cmatrix)\n",
    "    index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "    \n",
    "    aucs = []\n",
    "    classes_probs = []\n",
    "    classes_true_labels = []\n",
    "    for class_label in label_to_index.values():\n",
    "        class_label_verbose = index_to_label[class_label]\n",
    "        \n",
    "        class_true_labels = np.where(np.squeeze(true_labels) == class_label, 1, 0)\n",
    "        class_probs = probs[:, class_label]\n",
    "        classes_probs.append(class_probs)\n",
    "        classes_true_labels.append(class_true_labels)\n",
    "        \n",
    "        auc = roc_auc_score(class_true_labels, class_probs)\n",
    "        aucs.append(auc)\n",
    "        print(f\"class {class_label_verbose} AUC: {auc}\")\n",
    "        \n",
    "    return {\"accuracy\": accuracy, \"AUC_per_class\": aucs, \"plot\": plot,\n",
    "            \"classes_probs\": class_probs.tolist(), \"class_true_labels\": class_true_labels.tolist()}\n",
    "    \n",
    "\n",
    "def save_results(model, summary, description, results_dir=\"results\"):\n",
    "    \"\"\"Saves model, summary and plot on a drive\"\"\"\n",
    "    results_path = Path('results')\n",
    "    date_str = datetime.datetime.now().strftime(\"%Y%m%dT%H%M\")\n",
    "    folder_name = f\"results_{date_str}_{description}\"\n",
    "    results_folder_path = results_path / folder_name\n",
    "    results_folder_path.mkdir()\n",
    "    \n",
    "    summary[\"plot\"].savefig(results_folder_path / \"cmatrix.png\")\n",
    "    \n",
    "    summary_copy = summary.copy()\n",
    "    summary_copy.pop(\"plot\")\n",
    "    \n",
    "    model.save(results_folder_path / 'model.h5') \n",
    "    \n",
    "    with open(results_folder_path / \"evaluation.json\", \"w\") as f:\n",
    "        json.dump(summary_copy, f)\n",
    "    \n",
    "    def model_summary_to_file(summary):\n",
    "        with open(results_folder_path / \"model_summary.txt\", 'w') as f:\n",
    "            print(summary, file=f)\n",
    "\n",
    "    model.summary(print_fn=model_summary_to_file)\n",
    "    print(f\"Results saved in {results_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info, test_info = get_train_test_data_info(DATA_PATH)\n",
    "np.random.shuffle(train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [str(DATA_PATH / x[0]) for x in train_info]\n",
    "train_labels = [x[1] for x in train_info]\n",
    "\n",
    "test_paths = [str(DATA_PATH / x[0]) for x in test_info]\n",
    "test_labels = [x[1] for x in test_info]\n",
    "\n",
    "label_to_index = dict((name, index) for index, name in enumerate(set(train_labels)))\n",
    "\n",
    "train_labels = [np.array(label_to_index[x]) for x in train_labels]\n",
    "test_labels = [np.array(label_to_index[x]) for x in test_labels]\n",
    "\n",
    "def gen_X_train():\n",
    "    \"\"\"Returns generator of X_train values\"\"\"\n",
    "    for path in train_paths:\n",
    "        yield preprocess_data(path)\n",
    "        \n",
    "def gen_X_test():\n",
    "    \"\"\"Returns generator of X_test values\"\"\"\n",
    "    for path in test_paths:\n",
    "        yield preprocess_data(path)\n",
    "        \n",
    "def gen_y_train():\n",
    "    \"\"\"Returns generator of y train values\"\"\"\n",
    "    for label in train_labels:\n",
    "        yield label\n",
    "        \n",
    "def gen_y_test():\n",
    "    \"\"\"Returns generator of y test values\"\"\"\n",
    "    for label in test_labels:\n",
    "        yield label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0608 20:16:25.941871 140088214009664 deprecation.py:323] From /home/marcin/git/MGU-lstm/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "X_train = tf.data.Dataset.from_generator(gen_X_train, tf.float64)\n",
    "y_train = tf.data.Dataset.from_generator(gen_y_train, tf.int32)\n",
    "train_ds = tf.data.Dataset.zip((X_train, y_train))\n",
    "\n",
    "X_test = tf.data.Dataset.from_generator(gen_X_test, tf.float64)\n",
    "y_test = tf.data.Dataset.from_generator(gen_y_test, tf.int32)\n",
    "test_ds = tf.data.Dataset.zip((X_test, y_test))\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(buffer_size=500).batch(BATCH_SIZE).repeat().prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch > 20:\n",
    "        return 0.00001\n",
    "    elif epoch > 10:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.001\n",
    "    \n",
    "scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=0,\n",
    "                                              patience=3,\n",
    "                                              verbose=0, mode='auto')\n",
    "steps_per_epoch = np.ceil(len(train_labels)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0608 20:16:16.176132 140020782257984 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f588469f160>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0608 20:16:16.179924 140020782257984 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f588469f748>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(input_shape=(99, 161), filters=16, kernel_size=3, activation='selu', padding='same'),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='selu', padding='same'),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "  tf.keras.layers.Dense(len(ALLOWED_LABELS) + 1, activation=tf.keras.activations.softmax)\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, epochs=30, steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=test_ds, callbacks=[scheduler, early_stopping])\n",
    "\n",
    "preds = model.predict(test_ds)\n",
    "summary = get_summary(preds, test_labels)\n",
    "save_results(model, summary, \"small-bi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format b'Q\\x9b\\x8f\\x8c'... not understood.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8b4b25608030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data-kaggle/train/audio/five/53d5b86f_nohash_0.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/MGU-lstm/venv/lib/python3.6/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_big_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mfmt_chunk_received\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/MGU-lstm/venv/lib/python3.6/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36m_read_riff_chunk\u001b[0;34m(fid)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# There are also .wav files with \"FFIR\" or \"XFIR\" signatures?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         raise ValueError(\"File format {}... not \"\n\u001b[0;32m--> 168\u001b[0;31m                          \"understood.\".format(repr(str1)))\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Size of entire file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File format b'Q\\x9b\\x8f\\x8c'... not understood."
     ]
    }
   ],
   "source": [
    "wavfile.read(\"data-kaggle/train/audio/five/53d5b86f_nohash_0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgu-lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
